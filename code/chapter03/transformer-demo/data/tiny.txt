从早期的统计语言模型到大语言模型，科研人员进行了一系列的技术探索，从
而实现了模型能力的显著提升。下面将概括性地介绍一下大语言模型能够取得重
要进展背后的关键技术。具体的技术细节可以参考本书后续章节的详细介绍。
•规模扩展. 规模扩展是大语言模型的一个关键成功因素。在较早期的研究中，
OpenAI 从参数、数据、算力三个方面深入地研究了规模扩展对于模型性能所带来
的影响，建立了定量的函数关系，称之为“扩展法则”（Scaling Law）[15, 21]（论
文在 2020 年发表），并在 GPT-3 中探索了千亿级模型参数规模（175B 参数）所
带来的性能优势，为后期研发GPT 系列模型打下了重要的基础。随后，谷歌研究
团队也在2022 年推出了具有540B 参数的PaLM 模型，子公司DeepMind 也在同
年发表了重要研究成果—Chinchilla 扩展法则[22]。研究人员发现这些超大规模语
言模型能够展现出一些小型语言模型不具备的能力特点，如上下文学习能力、思
维链能力等[23–25]，这也成为区分上一代预训练语言模型与大语言模型的重要标
志。早期的研究主要关注模型参数规模所带来的性能优势，最近的工作则是加大
对于高质量数据的规模扩展。针对十亿级别（如2B 或7B）参数的模型使用超大
规模的数据（如2T 或3T 词元）进行训练，仍然可能无法达到这些模型的最大数
据容量。实现规模扩展的关键在于模型架构的可扩展性。Transformer 模型的可扩
展性非常强，对于硬件并行优化的支持也比较友好，特别适合大语言模型的研发，
很多工作也在进一步针对其进行优化与改进。
•数据工程. OpenAI 于2019 年就在GPT-2 的论文中[17] 给出了当前大语言模
型的技术路线图：通过在海量文本上进行下一个词预测的优化，使得模型能够学
习到丰富的语义知识信息，进而通过文本补全的方式解决各种下游任务。这种方
式最大的好处是，极大地简化了模型的设计与优化过程，使得模型训练与使用都
是基于自然语言生成的模式进行的。实际上，人工智能技术的几次重要升级都体
现出了这种“大道至简”的思想。例如，早期的深度学习系统通过端到端的训练方
法来建立输入与输出间的映射关系，而抛弃了传统耦合多个组件的复杂系统。在
这种通用的预训练范式下，模型能力本质上是来源于所见过的训练数据，因此数
据工程就变得极为重要，不是简单的扩大数据规模就能够实现的。